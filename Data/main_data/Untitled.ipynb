{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18da816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote CSVs to: C:\\Users\\harsh\\OneDrive\\Desktop\\Study_Materials\\College_Course\\Fall25\\ADT\\Project\\Data\\main_data\\clean_output\n",
      "lots_clean.csv rows: 169\n",
      "permits_clean.csv rows: 39\n",
      "lot_permit_clean.csv rows: 479\n",
      "SQL file with INSERTs: C:\\Users\\harsh\\OneDrive\\Desktop\\Study_Materials\\College_Course\\Fall25\\ADT\\Project\\Data\\main_data\\clean_output\\inserts.sql\n",
      "\n",
      "Next steps:\n",
      "1) Open the SQL file in MySQL Workbench and run it against your database (parking_proj).\n",
      "2) Or run: mysql -u <user> -p --local-infile=1 parking_proj < C:\\Users\\harsh\\OneDrive\\Desktop\\Study_Materials\\College_Course\\Fall25\\ADT\\Project\\Data\\main_data\\clean_output\\inserts.sql\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- config ----------\n",
    "INPUT_CSV = \"IUB_PARKING.csv\"   # exported from Excel (columns: TITLE, PERMITS, LATITUDE, LONGITUDE)\n",
    "OUT_DIR = Path(\"clean_output\")\n",
    "SEP = \"||\"  # separator used in your data\n",
    "MAX_TOKEN_LEN = 500\n",
    "# ----------------------------\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def safe_split_permits(s, sep=SEP):\n",
    "    \"\"\"Split permits string on sep safely, remove empty tokens, strip whitespace, and normalize quotes/backslashes.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    # Convert to str\n",
    "    s = str(s)\n",
    "    # Normalize common unicode quirks and newlines\n",
    "    s = s.replace('\\r', ' ').replace('\\n', ' ').strip()\n",
    "    # Replace multiple whitespace with single space\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    # Defensive: escape backslashes and keep quotes safe for CSV\n",
    "    s = s.replace('\\\\', '\\\\\\\\').replace('\"', '\\\"')\n",
    "    # Split on sep\n",
    "    parts = [p.strip() for p in s.split(sep)]\n",
    "    # Drop empties and very long garbage tokens\n",
    "    parts = [p for p in parts if p and len(p) <= MAX_TOKEN_LEN]\n",
    "    return parts\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize names for deduping (keeps original for output).\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    return re.sub(r'\\s+', ' ', str(name).strip())\n",
    "\n",
    "def sql_escape(val):\n",
    "    \"\"\"Escape single quotes for SQL insertion (basic).\"\"\"\n",
    "    if val is None:\n",
    "        return 'NULL'\n",
    "    s = str(val).replace(\"'\", \"''\")\n",
    "    return f\"'{s}'\"\n",
    "\n",
    "def main():\n",
    "    # load staging CSV\n",
    "    df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, na_values=[''])\n",
    "    # expected columns: TITLE, PERMITS, LATITUDE, LONGITUDE (case-insensitive)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    # Map columns\n",
    "    title_col = cols.get('title') or cols.get('lot') or df.columns[0]\n",
    "    permits_col = cols.get('permits') or df.columns[1] if len(df.columns) > 1 else df.columns[1] if len(df.columns) > 1 else df.columns[0]\n",
    "    lat_col = cols.get('latitude') or cols.get('lat') or None\n",
    "    lon_col = cols.get('longitude') or cols.get('lon') or None\n",
    "\n",
    "    # Clean lots dataframe (unique by title)\n",
    "    df['title_clean'] = df[title_col].astype(str).apply(normalize_name)\n",
    "    # Keep latitude/longitude if present and parse to numeric\n",
    "    if lat_col and lon_col and lat_col in df.columns and lon_col in df.columns:\n",
    "        df['latitude'] = pd.to_numeric(df[lat_col], errors='coerce')\n",
    "        df['longitude'] = pd.to_numeric(df[lon_col], errors='coerce')\n",
    "    else:\n",
    "        df['latitude'] = None\n",
    "        df['longitude'] = None\n",
    "\n",
    "    lots_df = df[['title_clean','latitude','longitude']].drop_duplicates(subset=['title_clean']).rename(\n",
    "        columns={'title_clean':'title'}\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Build permits set and lot_permit edges (title -> permit_name)\n",
    "    permits_set = set()\n",
    "    edges = []\n",
    "    for _, row in df.iterrows():\n",
    "        title = normalize_name(row['title_clean'])\n",
    "        raw_permits = row[permits_col] if permits_col in row else \"\"\n",
    "        parts = safe_split_permits(raw_permits, SEP)\n",
    "        for p in parts:\n",
    "            p_norm = normalize_name(p)\n",
    "            if not p_norm:\n",
    "                continue\n",
    "            permits_set.add(p_norm)\n",
    "            edges.append((title, p_norm))\n",
    "\n",
    "    # Create permits dataframe\n",
    "    permits_df = pd.DataFrame(sorted(list(permits_set)), columns=['name']).reset_index(drop=True)\n",
    "\n",
    "    # Create lot_permit df (title + permit_name)\n",
    "    lot_permit_df = pd.DataFrame(edges, columns=['title','permit_name']).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Export CSVs\n",
    "    lots_out = OUT_DIR / \"lots_clean.csv\"\n",
    "    permits_out = OUT_DIR / \"permits_clean.csv\"\n",
    "    lot_permit_out = OUT_DIR / \"lot_permit_clean.csv\"\n",
    "\n",
    "    lots_df.to_csv(lots_out, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    permits_df.to_csv(permits_out, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    lot_permit_df.to_csv(lot_permit_out, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    # Produce robust SQL file that ensures IDs are correct using INSERT ... SELECT\n",
    "    sql_out = OUT_DIR / \"inserts.sql\"\n",
    "    with open(sql_out, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\"-- Auto-generated SQL by clean_permits.py\\n\")\n",
    "        f.write(\"-- 1) Insert permits (id assigned by DB)\\n\")\n",
    "        for _, r in permits_df.iterrows():\n",
    "            name = r['name']\n",
    "            f.write(f\"INSERT IGNORE INTO permits (name) VALUES ({sql_escape(name)});\\n\")\n",
    "        f.write(\"\\n-- 2) Insert lots (id assigned by DB)\\n\")\n",
    "        for _, r in lots_df.iterrows():\n",
    "            title = r['title'] if pd.notna(r['title']) else ''\n",
    "            lat = r['latitude'] if pd.notna(r['latitude']) else None\n",
    "            lon = r['longitude'] if pd.notna(r['longitude']) else None\n",
    "            lat_sql = 'NULL' if lat is None or str(lat).strip()=='' else str(lat)\n",
    "            lon_sql = 'NULL' if lon is None or str(lon).strip()=='' else str(lon)\n",
    "            f.write(f\"INSERT IGNORE INTO lots (title, latitude, longitude) VALUES ({sql_escape(title)}, {lat_sql}, {lon_sql});\\n\")\n",
    "        f.write(\"\\n-- 3) Insert lot_permit by joining inserted lots & permits to get correct IDs\\n\")\n",
    "        f.write(\"-- This guarantees lot_id and permit_id are the actual DB-assigned keys.\\n\")\n",
    "        # For each mapping generate an INSERT ... SELECT to join by title/name\n",
    "        for _, r in lot_permit_df.iterrows():\n",
    "            title = r['title']\n",
    "            permit = r['permit_name']\n",
    "            # Use INSERT IGNORE to avoid duplicates; join on exact title/name equality (trim in DB if necessary)\n",
    "            f.write(\n",
    "                \"INSERT IGNORE INTO lot_permit (lot_id, permit_id)\\n\"\n",
    "                f\"SELECT l.lot_id, p.permit_id FROM lots l JOIN permits p ON p.name = {sql_escape(permit)}\\n\"\n",
    "                f\"WHERE l.title = {sql_escape(title)};\\n\"\n",
    "            )\n",
    "        f.write(\"\\n-- End of generated SQL\\n\")\n",
    "\n",
    "    # Print previews and paths\n",
    "    print(\"Wrote CSVs to:\", OUT_DIR.resolve())\n",
    "    print(\"lots_clean.csv rows:\", len(lots_df))\n",
    "    print(\"permits_clean.csv rows:\", len(permits_df))\n",
    "    print(\"lot_permit_clean.csv rows:\", len(lot_permit_df))\n",
    "    print(\"SQL file with INSERTs:\", sql_out.resolve())\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1) Open the SQL file in MySQL Workbench and run it against your database (parking_proj).\")\n",
    "    print(\"2) Or run: mysql -u <user> -p --local-infile=1 parking_proj <\", sql_out.resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5344209f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dest</th>\n",
       "      <th>distance_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atwater Parking Garage</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>1418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fee Lane Parking Garage</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forrest Ave Parking Garage</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headley School Parking</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>1891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Henderson Parking Garage</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                dest  distance_value\n",
       "0      Atwater Parking Garage  1000 N INDIANA AVE            1418\n",
       "1     Fee Lane Parking Garage  1000 N INDIANA AVE             760\n",
       "2  Forrest Ave Parking Garage  1000 N INDIANA AVE             510\n",
       "3      Headley School Parking  1000 N INDIANA AVE            1891\n",
       "4    Henderson Parking Garage  1000 N INDIANA AVE            1151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\harsh\\OneDrive\\Desktop\\Study_Materials\\College_Course\\Fall25\\ADT\\Project\\Data\\main_data\\building_parking_distance_matrix.csv\")\n",
    "\n",
    "# Rename first column to src (it was \"Unnamed: 0\")\n",
    "df.rename(columns={df.columns[0]: \"src\"}, inplace=True)\n",
    "\n",
    "# Convert wide â†’ long format\n",
    "long_df = df.melt(\n",
    "    id_vars=\"src\",             # column to keep\n",
    "    var_name=\"dest\",            # new column name for headers\n",
    "    value_name=\"distance_value\"  # new column name for cell values\n",
    ")\n",
    "\n",
    "# Save to new CSV (optional)\n",
    "long_df.to_csv(\"building_parking_distance_long.csv\", index=False)\n",
    "\n",
    "long_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b301b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\harsh\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp39-cp39-win_amd64.whl (11.4 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\harsh\\\\anaconda3\\\\Lib\\\\site-packages\\\\~andas.libs\\\\msvcp140-0f2ea95580b32bcfc81c235d5751ce78.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c867de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://root:root@host:3306/adt_project\")\n",
    "\n",
    "df = pd.read_csv(\"clean_output//building_parking_distance_long.csv\")  # ensure columns match staging\n",
    "# df.to_sql(\"staging_lot_building_distance\", engine, if_exists=\"append\", index=False, chunksize=5000, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2004dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lot_title_raw</th>\n",
       "      <th>building_name_raw</th>\n",
       "      <th>distance_sec_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lot 442</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>1418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lot 255</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lot 238</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headley School Parking</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>1891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lot 424</td>\n",
       "      <td>1000 N INDIANA AVE</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            lot_title_raw   building_name_raw  distance_sec_raw\n",
       "0                 Lot 442  1000 N INDIANA AVE              1418\n",
       "1                 Lot 255  1000 N INDIANA AVE               760\n",
       "2                 Lot 238  1000 N INDIANA AVE               510\n",
       "3  Headley School Parking  1000 N INDIANA AVE              1891\n",
       "4                 Lot 424  1000 N INDIANA AVE              1151"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161f7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c93a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'user': 'root',\n",
    "  'password': 'admin',\n",
    "  'host': '127.0.0.1',\n",
    "  'port': 3306,\n",
    "  'database': 'adt_project',\n",
    "  'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "db_connect=mysql.connector.connect(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c037ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "df = pd.read_csv(r\"clean_output\\building_parking_distance_long.csv\")\n",
    "\n",
    "conn = mysql.connector.connect(**config)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# optional: clear staging\n",
    "cur.execute(\"TRUNCATE TABLE staging_lot_building_distance\")\n",
    "conn.commit()\n",
    "\n",
    "insert_sql = \"\"\"\n",
    "INSERT INTO staging_lot_building_distance\n",
    "(lot_title_raw, building_name_raw, distance_sec_raw)\n",
    "VALUES (%s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "data = list(df.itertuples(index=False, name=None))\n",
    "\n",
    "chunk = 5000\n",
    "for i in range(0, len(data), chunk):\n",
    "    cur.executemany(insert_sql, data[i:i+chunk])\n",
    "    conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89bb1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
